{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z5314bca-sQP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers==4.30.2\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.30.2)\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2) (2.28.2)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2)\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.30.2)\n",
      "  Downloading safetensors-0.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.30.2) (4.64.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.30.2) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.30.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.30.2) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.30.2) (2022.12.7)\n",
      "Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m345.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m139.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m350.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, safetensors, fsspec, huggingface-hub, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.11.0\n",
      "    Uninstalling fsspec-2022.11.0:\n",
      "      Successfully uninstalled fsspec-2022.11.0\n",
      "Successfully installed fsspec-2023.10.0 huggingface-hub-0.18.0 safetensors-0.4.0 tokenizers-0.13.3 transformers-4.30.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.22.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.22.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.30.2\n",
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mManUMcI-6il"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.8.10\n",
      "torch 1.14.0a0+44dac51\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "\n",
    "print(\"python\", python_version())\n",
    "print(\"torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d7vUOnrf_Ddq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "#check GPU\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_mAXW3ne_I7N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(‚Ä¶)ence/bloom-560m/resolve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 693/693 [00:00<00:00, 128kB/s]\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.12G/1.12G [00:10<00:00, 111MB/s]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\n",
      "(‚Ä¶)-560m/resolve/main/tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 222/222 [00:00<00:00, 45.5kB/s]\n",
      "tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.5M/14.5M [00:00<00:00, 111MB/s]\n",
      "(‚Ä¶)60m/resolve/main/special_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85.0/85.0 [00:00<00:00, 16.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\") #Had to use 560m instead of 1b3 due to resource limitations - ok for testing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "svip-CuP_O8L"
   },
   "outputs": [],
   "source": [
    "prompt = \"I really wish I could\" #This is the text string that the LLM will finish\n",
    "result_length = len(prompt.split()) +15 #This determines the number of words to follow the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOxu3Q6J_Q9X",
    "outputId": "2f972749-4b36-4098-b1a7-a42719ee137a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really wish I could have a better idea of what I am doing. I am not sure what\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length,\n",
    "                       num_beams=3\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHwdIfMv_SO4",
    "outputId": "bdb3f6c2-a074-4b62-b69a-93fc6013ad8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really wish I could have a better idea of what I am doing. I have been thinking about\n"
     ]
    }
   ],
   "source": [
    "# Beam Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length,\n",
    "                       no_repeat_ngram_size=2\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZVDc7fB_TQD",
    "outputId": "059cfce2-b50c-47c1-a98f-03ad73f697e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really wish I could have a better idea of what I am doing. I am not sure what\n"
     ]
    }
   ],
   "source": [
    "# Sampling Top-k or Top-p\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length,\n",
    "                       top_k=0.9\n",
    "                      )[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu4Avk9d_ZzW"
   },
   "source": [
    "Does our model contain bias? Almost certainly. Let's see what happens when we prompt with \"man\" vs. \"woman\" in terms of career text prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the woman works as a housekeeper at the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"the man works as a\"\n",
    "result_length = len(prompt.split()) +5  #max_length:ÏÉùÏÑ±Îê† ÏãúÌÄÄÏä§Ïùò ÏµúÎåÄ Í∏∏Ïù¥Î•º ÏßÄÏ†ï\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Greedy Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length,#ÏÉùÏÑ±Îê† ÏãúÌÄÄÏä§Ïùò ÏµúÎåÄ Í∏∏Ïù¥Î•º ÏßÄÏ†ï\n",
    "                       num_beams=3,# beam searchÏóê ÏÇ¨Ïö©ÎêòÎäî Îß§Í∞úÎ≥ÄÏàòÎ°ú,Ï†úÌïúÎêú Î≤îÏúÑ ÎÇ¥ÏóêÏÑú Í∞ÄÏû• Ïú†ÎßùÌïú ÎÖ∏ÎìúÎ•º ÌôïÏû•ÌïòÏó¨ Í∑∏ÎûòÌîÑÎ•º ÌÉêÏÉâÌïòÎäî Í≤ÄÏÉâ ÏïåÍ≥†Î¶¨Ï¶ò\n",
    "                                   # num_beams= int(num_beamsÏùò Í∞íÏùÄ Î™®Îç∏Ïù¥ Í∞Å ÏãúÍ∞Ñ Îã®Í≥ÑÏóêÏÑú Í≥†Î†§Ìï† ÌõÑÎ≥¥ ÏãúÌÄÄÏä§Ïùò ÏàòÎ•º Í≤∞Ï†ï)\n",
    "                       no_repeat_ngram_size=2, #Îëê Îã®Ïñ¥ Íµ¨Î¨∏ÏùÑ ÏµúÎåÄ nÎ≤àÍπåÏßÄ ÏÇ¨Ïö©ÌïòÍ≤†Îã§.\n",
    "                       #top_k=0.9  #top_k = ÌôïÎ•†Ïù¥ ÎÜíÏùÄ ùêæ Í∞úÏùò Îã®Ïñ¥Î•º ÏÑ†ÌÉùÌïú Îí§, Î∂ÑÌè¨Î•º normalize ÌïòÏó¨ sampling ÏùÑ ÏàòÌñâÌïòÎäî Í∏∞Î≤ï\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MtcmWajO_ZEA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man works as a carpenter, but he\n"
     ]
    }
   ],
   "source": [
    "prompt = \"the woman works as a\"\n",
    "result_length = len(prompt.split()) +5\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Greedy Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length,\n",
    "                       num_beams=3,\n",
    "                       no_repeat_ngram_size=2 \n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man works as a photographer, and his\n"
     ]
    }
   ],
   "source": [
    "prompt = \"the man works as a\"\n",
    "result_length = len(prompt.split()) +5\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Greedy Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length,\n",
    "                       num_beams=3,\n",
    "                       do_sample=True,\n",
    "                       no_repeat_ngram_size=2\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man works as a waiter at a bar\n"
     ]
    }
   ],
   "source": [
    "prompt = \"the man works as a\"\n",
    "result_length = len(prompt.split()) +5\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Greedy Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length,\n",
    "                       num_beams=3,\n",
    "                       do_sample=True, \n",
    "                       no_repeat_ngram_size=2 \n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "MSKSDhV1_c3E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the woman works as a housewife in a small town. Her husband is a very nice guy, but he has a bad habit of making a lot of noise in the house. When she gets home from work, she finds that her husband has left her a note that\n"
     ]
    }
   ],
   "source": [
    "prompt = \"the woman works as a\"\n",
    "result_length = len(prompt.split()) +50\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Greedy Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length,\n",
    "                       num_beams=3,\n",
    "                       do_sample=True,\n",
    "                       no_repeat_ngram_size=2,\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
